{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOq0iNwOB4J+qi67Ogg4pz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dangoln/HAR-using-DAGHAR/blob/main/CNN_LSTM_on_DAGHAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xEtfmx2Bvmq",
        "outputId": "b5cd35b0-efd5-44de-eb5a-5523e9a40f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Mount to the default /content/drive directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4jwCMqCcGS",
        "outputId": "9b79441a-0533-44b9-93e7-99fce42d9b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "Nhzezbq1Cfgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "train_path = \"/content/drive/My Drive/HAR-Datasets/DAGHAR/standardized/RealWorld/RealWorld_thigh/train.csv\"\n",
        "val_path = \"/content/drive/My Drive/HAR-Datasets/DAGHAR/standardized/RealWorld/RealWorld_thigh/validation.csv\"\n",
        "test_path = \"/content/drive/My Drive/HAR-Datasets/DAGHAR/standardized/RealWorld/RealWorld_thigh/test.csv\""
      ],
      "metadata": {
        "id": "RrtJ1_BqDxxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_dim = 3  # Number of sensor channels (adjust based on dataset)\n",
        "num_classes = 6  # Number of activity classes\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "sequence_length = 128  # Define based on dataset characteristics"
      ],
      "metadata": {
        "id": "8ZtLx3h1D0rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-BiLSTM Model\n",
        "class CNN_BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(CNN_BiLSTM, self).__init__()\n",
        "\n",
        "        # Adjusted in_channels to 128 and out_channels to 64\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(128 * 2, num_classes)  # Adjusted for bidirectional LSTM\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv1d expects input shape: [batch_size, channels, sequence_length]\n",
        "        # The input shape might be [batch_size, sequence_length, channels], so we need to transpose\n",
        "        x = x.transpose(1, 2)  # Transpose to [batch_size, channels, sequence_length]\n",
        "        x = self.conv1(x)  # Apply convolution\n",
        "        x = self.pool(x)   # Apply max pooling\n",
        "        x = x.transpose(1, 2)  # LSTM expects shape [batch_size, seq_len, input_size]\n",
        "        x, _ = self.lstm(x)  # Apply LSTM\n",
        "        x = x[:, -1, :]  # Get the output of the last time step\n",
        "        x = self.fc(x)  # Fully connected layer\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "J_hOGfgmD2Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom dataset\n",
        "class HAR_Dataset(Dataset):\n",
        "    def __init__(self, file_path, sequence_length):\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Identify and drop non-numeric columns\n",
        "        df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "        # Separate features and labels (assuming last column is activity labels)\n",
        "        self.data = df.iloc[:, :-1].values  # Sensor features\n",
        "        self.labels = df.iloc[:, -1].values  # Activity labels\n",
        "\n",
        "        # Convert labels to integers if they are strings\n",
        "        if isinstance(self.labels[0], str):\n",
        "            label_mapping = {label: idx for idx, label in enumerate(np.unique(self.labels))}\n",
        "            self.labels = np.array([label_mapping[label] for label in self.labels])\n",
        "\n",
        "        # Normalize sensor data\n",
        "        scaler = StandardScaler()\n",
        "        self.data = scaler.fit_transform(self.data)\n",
        "\n",
        "        # Reshape to (samples, sequence_length, features)\n",
        "        num_samples = self.data.shape[0] // sequence_length\n",
        "        self.data = self.data[:num_samples * sequence_length].reshape(num_samples, sequence_length, -1)\n",
        "        self.labels = self.labels[:num_samples * sequence_length:sequence_length]  # One label per sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx], dtype=torch.float32).permute(1, 0), torch.tensor(self.labels[idx], dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "uL4lrtBEGQgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = HAR_Dataset(train_path, sequence_length)\n",
        "val_dataset = HAR_Dataset(val_path, sequence_length)\n",
        "test_dataset = HAR_Dataset(test_path, sequence_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "zHzskzDLD5v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = 128  # Adjust as per your actual input data\n",
        "num_classes = 6  # Example number of classes\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = CNN_BiLSTM(input_dim, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Assuming train_loader and train_dataset are already defined\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update loss and correct predictions\n",
        "        total_loss += loss.item()\n",
        "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwYRtoBVGfEI",
        "outputId": "54a76179-9723-43ca-ad31-b5f47936dbbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 3.5812, Accuracy: 0.0875\n",
            "Epoch [2/10], Loss: 3.5169, Accuracy: 0.3375\n",
            "Epoch [3/10], Loss: 3.4514, Accuracy: 0.3125\n",
            "Epoch [4/10], Loss: 3.3654, Accuracy: 0.4875\n",
            "Epoch [5/10], Loss: 3.2092, Accuracy: 0.5250\n",
            "Epoch [6/10], Loss: 2.8483, Accuracy: 0.5500\n",
            "Epoch [7/10], Loss: 2.6519, Accuracy: 0.5875\n",
            "Epoch [8/10], Loss: 2.2528, Accuracy: 0.7625\n",
            "Epoch [9/10], Loss: 1.7678, Accuracy: 0.8750\n",
            "Epoch [10/10], Loss: 1.3010, Accuracy: 0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "    return correct / len(dataloader.dataset)\n",
        "\n",
        "# Validation & Test\n",
        "val_acc = evaluate(model, val_loader)\n",
        "test_acc = evaluate(model, test_loader)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKffXxZGG1Qt",
        "outputId": "54ba1681-001b-4cac-9422-dc0472543129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2143\n",
            "Test Accuracy: 0.4545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "bgXr6WnoJwCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test val+test sets\n",
        "val_path1 = \"/content/drive/My Drive/HAR-Datasets/DAGHAR/standardized/RealWorld/RealWorld_thigh/validation.csv\"\n",
        "test_path1 = \"/content/drive/My Drive/HAR-Datasets/DAGHAR/standardized/RealWorld/RealWorld_upperarm/validation.csv\"\n",
        "\n",
        "val_dataset1 = HAR_Dataset(val_path1, sequence_length)\n",
        "test_dataset1 = HAR_Dataset(test_path1, sequence_length)\n",
        "\n",
        "val_loader1 = DataLoader(val_dataset1, batch_size=batch_size, shuffle=False)\n",
        "test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Validation & Test\n",
        "val_acc1 = evaluate(model, val_loader1)\n",
        "test_acc1 = evaluate(model, test_loader1)\n",
        "\n",
        "print(f\"Validation Accuracy: {val_acc1:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2kLo5r0JwrG",
        "outputId": "562f9c49-0b4f-474f-a0c1-4e25fe1e8944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2143\n",
            "Test Accuracy: 0.2143\n"
          ]
        }
      ]
    }
  ]
}